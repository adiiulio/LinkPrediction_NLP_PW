{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1dcee94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5303db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FB15k-237 from HuggingFace\n",
    "dataset = load_dataset(\"VLyb/FB15k\")\n",
    "\n",
    "train_triples = dataset[\"train\"]\n",
    "test_triples = dataset[\"test\"]\n",
    "valid_triples = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7defcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'head': '/m/027rn', 'relation': '/location/country/form_of_government', 'tail': '/m/06cx9'}\n"
     ]
    }
   ],
   "source": [
    "print(train_triples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "095b21fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"answerdotai/ModernBERT-base\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c2f26bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModernBertModel(\n",
       "  (embeddings): ModernBertEmbeddings(\n",
       "    (tok_embeddings): Embedding(50368, 768, padding_idx=50283)\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0): ModernBertEncoderLayer(\n",
       "      (attn_norm): Identity()\n",
       "      (attn): ModernBertAttention(\n",
       "        (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (rotary_emb): ModernBertRotaryEmbedding()\n",
       "        (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_drop): Identity()\n",
       "      )\n",
       "      (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): ModernBertMLP(\n",
       "        (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (act): GELUActivation()\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1-21): 21 x ModernBertEncoderLayer(\n",
       "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): ModernBertAttention(\n",
       "        (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (rotary_emb): ModernBertRotaryEmbedding()\n",
       "        (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_drop): Identity()\n",
       "      )\n",
       "      (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): ModernBertMLP(\n",
       "        (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (act): GELUActivation()\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a729e615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/m/027rn /location/country/form_of_government /m/06cx9.\n"
     ]
    }
   ],
   "source": [
    "def triple_to_sentence2(triple):\n",
    "    h = triple['head']\n",
    "    r = triple['relation']\n",
    "    t = triple['tail']\n",
    "    return f\"{h} {r} {t}.\"\n",
    "\n",
    "def triple_to_sentence(h, r, t):\n",
    "    return f\"{h} {r} {t}.\"\n",
    "\n",
    "s = triple_to_sentence2(train_triples[0])\n",
    "print(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d581705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def score_sentence(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # shape: [1, hidden_size]\n",
    "    score = torch.norm(cls_embedding, p=2, dim=1)  # simple scalar score\n",
    "    return score.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3ff15e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a set containing all the entities in any of the three sets\n",
    "all_entities = set()\n",
    "\n",
    "for split in [train_triples, test_triples, valid_triples]:\n",
    "    for row in split:\n",
    "        all_entities.update([row[\"head\"], row[\"tail\"]])\n",
    "\n",
    "all_entities = list(all_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f960e939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# corrupt either the head or the tail\n",
    "def corrupt_triple(triple, entity_list):\n",
    "    head, relation, tail = triple\n",
    "    if random.random() < 0.5:\n",
    "        # Corrupt tail\n",
    "        corrupted = (head, relation, random.choice(entity_list))\n",
    "    else:\n",
    "        # Corrupt head\n",
    "        corrupted = (random.choice(entity_list), relation, tail)\n",
    "    return corrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9cd9dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive score: 37.4577\n",
      "Negative score: 37.3436\n"
     ]
    }
   ],
   "source": [
    "# Pick a positive triple\n",
    "pos = test_triples[0]\n",
    "pos_triple = (pos[\"head\"], pos[\"relation\"], pos[\"tail\"])\n",
    "\n",
    "# Corrupt it\n",
    "neg_triple = corrupt_triple(pos_triple, all_entities)\n",
    "\n",
    "# Convert to sentences\n",
    "pos_sentence = triple_to_sentence(*pos_triple)\n",
    "neg_sentence = triple_to_sentence(*neg_triple)\n",
    "\n",
    "# Score both\n",
    "pos_score = score_sentence(pos_sentence)\n",
    "neg_score = score_sentence(neg_sentence)\n",
    "\n",
    "print(f\"Positive score: {pos_score:.4f}\")\n",
    "print(f\"Negative score: {neg_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db7955c",
   "metadata": {},
   "source": [
    "using bert alone, with a basic loss function and no training shows that the model is not attuned enough to this type of task and the difference between positive (indicating that the triple is correct) and negative (triple is incorrect) scores is too mild for it to be acceptable. Therefore some steps need to be taken in order to make it better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ee3514",
   "metadata": {},
   "source": [
    "- build a dataset which contains the triples labelled (corrupted or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "78e7ff16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "483142"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "92b31759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'head': '/m/027rn',\n",
       " 'relation': '/location/country/form_of_government',\n",
       " 'tail': '/m/06cx9'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_triples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4fc248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# corrupt either the head or the tail\n",
    "def corrupt_triple2(triple, entity_list):\n",
    "    h, r, t = triple['head'], triple['relation'], triple['tail']\n",
    "    if random.random() < 0.5:\n",
    "        corrupted = (random.choice(entity_list), r, t)\n",
    "    else:\n",
    "        corrupted = (h, r, random.choice(entity_list))\n",
    "    return corrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe1d244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset(\"VLyb/FB15k\")\n",
    "# train_triples = dataset[\"train\"]\n",
    "\n",
    "N = 2000 # let us label a subset\n",
    "data = []\n",
    "for i in range(1, N+1):\n",
    "    triple = train_triples[i]\n",
    "    h, r, t = triple[\"head\"], triple[\"relation\"], triple[\"tail\"]\n",
    "    data.append((triple_to_sentence(h, r, t), 1))\n",
    "\n",
    "    ch, cr, ct = corrupt_triple2(triple, all_entities)\n",
    "    data.append((triple_to_sentence(ch, cr, ct), 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea47aeb9",
   "metadata": {},
   "source": [
    "- create a torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1fd0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# # class TripleDataset(Dataset):\n",
    "# #     def __init__(self, triples):\n",
    "# #         self.triples = triples\n",
    "\n",
    "# #     def __len__(self):\n",
    "# #         return len(self.triples)\n",
    "\n",
    "# #     def __getitem__(self, idx):\n",
    "# #         sentence, label = self.triples[idx]\n",
    "# #         return tokenizer(sentence, return_tensors='pt', truncation=True, padding='max_length', max_length=32), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "# class TripleDataset(Dataset):\n",
    "#     def __init__(self, triples):\n",
    "#         self.triples = triples\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.triples)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         sentence, label = self.triples[idx]\n",
    "#         encoding = tokenizer(\n",
    "#             sentence,\n",
    "#             truncation=True,\n",
    "#             padding='max_length',\n",
    "#             max_length=32,\n",
    "#             return_tensors='pt'\n",
    "#         )\n",
    "\n",
    "#         return {\n",
    "#             \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "#             \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "#             \"label\": torch.tensor(label, dtype=torch.float)\n",
    "#         }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "97f49369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class TripleDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence, label = self.data[idx]\n",
    "        encoded = self.tokenizer(sentence, truncation=True, padding=\"max_length\", max_length=32, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\", trust_remote_code=True)\n",
    "\n",
    "dataset = TripleDataset(data, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010e3c11",
   "metadata": {},
   "source": [
    "- add classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "18814b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# class TripleClassifier(nn.Module):\n",
    "#     def __init__(self, base_model):\n",
    "#         super().__init__()\n",
    "#         self.encoder = base_model\n",
    "#         self.classifier = nn.Linear(base_model.config.hidden_size, 1)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         cls = outputs.last_hidden_state[:, 0, :]  # [CLS]\n",
    "#         logits = self.classifier(cls).squeeze(-1)\n",
    "#         return logits\n",
    "\n",
    "# class TripleClassifier(nn.Module):\n",
    "#     def __init__(self, base_model):\n",
    "#         super().__init__()\n",
    "#         self.encoder = base_model\n",
    "#         self.classifier = nn.Linear(base_model.config.hidden_size, 1)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         cls_embedding = outputs.last_hidden_state[:, 0, :]  # shape: (batch_size, hidden_size)\n",
    "#         logits = self.classifier(cls_embedding)  # shape: (batch_size, 1)\n",
    "#         return logits.squeeze(1)  # shape: (batch_size,)\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class TripleClassifier(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.encoder = base_model\n",
    "        self.classifier = nn.Linear(base_model.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(cls).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9f0eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModel.from_pretrained(\"answerdotai/ModernBERT-base\", trust_remote_code=True)\n",
    "model = TripleClassifier(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8085cd",
   "metadata": {},
   "source": [
    "- train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98544831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # Create an instance of your dataset\n",
    "# dataset = TripleDataset(data)  # 'data' should be your list of (sentence, label) tuples\n",
    "\n",
    "# # Create the DataLoader\n",
    "# dataloader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=16,         # You can change this depending on memory\n",
    "#     shuffle=True,          # Shuffle for training\n",
    "#     drop_last=True         # Optional: drop last batch if it's incomplete\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "08662f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 174.9417\n",
      "Epoch 2 Loss: 174.6750\n",
      "Epoch 3 Loss: 174.1778\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model.train()\n",
    "\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
